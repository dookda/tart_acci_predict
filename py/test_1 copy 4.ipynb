{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[22:40:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
                        "Parameters: { \"silent\" } might not be used.\n",
                        "\n",
                        "  This may not be accurate due to some parameters are only used in language bindings but\n",
                        "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
                        "  verification. Please open an issue if you find above cases.\n",
                        "\n",
                        "\n",
                        "[22:40:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
                        "[0]\teval-logloss:0.22669\ttrain-logloss:0.23338\n",
                        "[1]\teval-logloss:0.13787\ttrain-logloss:0.13666\n",
                        "error=0.021726\n",
                        "start running example of build DMatrix from scipy.sparse CSR Matrix\n",
                        "[22:40:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
                        "Parameters: { \"silent\" } might not be used.\n",
                        "\n",
                        "  This may not be accurate due to some parameters are only used in language bindings but\n",
                        "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
                        "  verification. Please open an issue if you find above cases.\n",
                        "\n",
                        "\n",
                        "[22:40:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
                        "[0]\teval-logloss:0.22669\ttrain-logloss:0.23338\n",
                        "[1]\teval-logloss:0.13787\ttrain-logloss:0.13666\n",
                        "start running example of build DMatrix from scipy.sparse CSC Matrix\n",
                        "[22:40:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
                        "Parameters: { \"silent\" } might not be used.\n",
                        "\n",
                        "  This may not be accurate due to some parameters are only used in language bindings but\n",
                        "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
                        "  verification. Please open an issue if you find above cases.\n",
                        "\n",
                        "\n",
                        "[22:40:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
                        "[0]\teval-logloss:0.22669\ttrain-logloss:0.23338\n",
                        "[1]\teval-logloss:0.13787\ttrain-logloss:0.13666\n",
                        "start running example of build DMatrix from numpy array\n",
                        "[22:40:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
                        "Parameters: { \"silent\" } might not be used.\n",
                        "\n",
                        "  This may not be accurate due to some parameters are only used in language bindings but\n",
                        "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
                        "  verification. Please open an issue if you find above cases.\n",
                        "\n",
                        "\n",
                        "[22:40:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
                        "[0]\teval-logloss:0.22669\ttrain-logloss:0.23338\n",
                        "[1]\teval-logloss:0.13787\ttrain-logloss:0.13666\n",
                        "end\n"
                    ]
                }
            ],
            "source": [
                "import numpy as np\n",
                "import scipy.sparse\n",
                "import pickle\n",
                "import xgboost as xgb\n",
                "\n",
                "train = r'E:\\StudyLAB\\1_2564\\499\\xgboost\\train.txt.txt'\n",
                "featmap = r'E:\\StudyLAB\\1_2564\\499\\xgboost\\featmap.txt'\n",
                "test = r'E:\\StudyLAB\\1_2564\\499\\xgboost\\test.txt.txt'\n",
                "\n",
                "### simple example\n",
                "# load file from text file, also binary buffer generated by xgboost\n",
                "dtrain = xgb.DMatrix(train)\n",
                "dtest = xgb.DMatrix(test)\n",
                "\n",
                "# specify parameters via map, definition are same as c++ version\n",
                "param = {'max_depth':2, 'eta':1, 'silent':1, 'objective':'binary:logistic'}\n",
                "\n",
                "# specify validations set to watch performance\n",
                "watchlist = [(dtest, 'eval'), (dtrain, 'train')]\n",
                "num_round = 2\n",
                "bst = xgb.train(param, dtrain, num_round, watchlist)\n",
                "\n",
                "# this is prediction\n",
                "preds = bst.predict(dtest)\n",
                "labels = dtest.get_label()\n",
                "print('error=%f' % (sum(1 for i in range(len(preds)) if int(preds[i] > 0.5) != labels[i]) / float(len(preds))))\n",
                "bst.save_model('0001.model')\n",
                "# dump model\n",
                "bst.dump_model('dump.raw.txt')\n",
                "# dump model with feature map\n",
                "bst.dump_model('featmap')\n",
                "\n",
                "# save dmatrix into binary buffer\n",
                "dtest.save_binary('dtest.buffer')\n",
                "# save model\n",
                "bst.save_model('xgb.model')\n",
                "# load model and data in\n",
                "bst2 = xgb.Booster(model_file='xgb.model')\n",
                "dtest2 = xgb.DMatrix('dtest.buffer')\n",
                "preds2 = bst2.predict(dtest2)\n",
                "# assert they are the same\n",
                "assert np.sum(np.abs(preds2 - preds)) == 0\n",
                "\n",
                "# alternatively, you can pickle the booster\n",
                "pks = pickle.dumps(bst2)\n",
                "# load model and data in\n",
                "bst3 = pickle.loads(pks)\n",
                "preds3 = bst3.predict(dtest2)\n",
                "# assert they are the same\n",
                "assert np.sum(np.abs(preds3 - preds)) == 0\n",
                "\n",
                "###\n",
                "# build dmatrix from scipy.sparse\n",
                "print('start running example of build DMatrix from scipy.sparse CSR Matrix')\n",
                "labels = []\n",
                "row = []; col = []; dat = []\n",
                "i = 0\n",
                "for l in open(train):\n",
                "    arr = l.split()\n",
                "    labels.append(int(arr[0]))\n",
                "    for it in arr[1:]:\n",
                "        k,v = it.split(':')\n",
                "        row.append(i); col.append(int(k)); dat.append(float(v))\n",
                "    i += 1\n",
                "csr = scipy.sparse.csr_matrix((dat, (row, col)))\n",
                "dtrain = xgb.DMatrix(csr, label=labels)\n",
                "watchlist = [(dtest, 'eval'), (dtrain, 'train')]\n",
                "bst = xgb.train(param, dtrain, num_round, watchlist)\n",
                "\n",
                "print('start running example of build DMatrix from scipy.sparse CSC Matrix')\n",
                "# we can also construct from csc matrix\n",
                "csc = scipy.sparse.csc_matrix((dat, (row, col)))\n",
                "dtrain = xgb.DMatrix(csc, label=labels)\n",
                "watchlist = [(dtest, 'eval'), (dtrain, 'train')]\n",
                "bst = xgb.train(param, dtrain, num_round, watchlist)\n",
                "\n",
                "print('start running example of build DMatrix from numpy array')\n",
                "# NOTE: npymat is numpy array, we will convert it into scipy.sparse.csr_matrix in internal implementation\n",
                "# then convert to DMatrix\n",
                "npymat = csr.todense()\n",
                "dtrain = xgb.DMatrix(npymat, label=labels)\n",
                "watchlist = [(dtest, 'eval'), (dtrain, 'train')]\n",
                "bst = xgb.train(param, dtrain, num_round, watchlist)\n",
                "print('end')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "start running example to start from a initial prediction\n",
                        "[22:44:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
                        "Parameters: { \"silent\" } might not be used.\n",
                        "\n",
                        "  This may not be accurate due to some parameters are only used in language bindings but\n",
                        "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
                        "  verification. Please open an issue if you find above cases.\n",
                        "\n",
                        "\n",
                        "[22:44:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
                        "[0]\teval-logloss:0.22669\ttrain-logloss:0.23338\n",
                        "this is result of running from initial prediction\n",
                        "[22:44:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
                        "Parameters: { \"silent\" } might not be used.\n",
                        "\n",
                        "  This may not be accurate due to some parameters are only used in language bindings but\n",
                        "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
                        "  verification. Please open an issue if you find above cases.\n",
                        "\n",
                        "\n",
                        "[22:44:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
                        "[0]\teval-logloss:0.13787\ttrain-logloss:0.13666\n",
                        "<xgboost.core.Booster object at 0x0000026D9568D4C0>\n",
                        "<xgboost.core.Booster object at 0x0000026D956AE3A0>\n"
                    ]
                }
            ],
            "source": [
                "dtrain = xgb.DMatrix(train)\n",
                "dtest = xgb.DMatrix(test)\n",
                "watchlist = [(dtest, 'eval'), (dtrain, 'train')]\n",
                "###\n",
                "# advanced: start from a initial base prediction\n",
                "#\n",
                "print ('start running example to start from a initial prediction')\n",
                "# specify parameters via map, definition are same as c++ version\n",
                "param = {'max_depth':2, 'eta':1, 'silent':1, 'objective':'binary:logistic'}\n",
                "# train xgboost for 1 round\n",
                "bst = xgb.train(param, dtrain, 1, watchlist)\n",
                "# Note: we need the margin value instead of transformed prediction in set_base_margin\n",
                "# do predict with output_margin=True, will always give you margin values before logistic transformation\n",
                "ptrain = bst.predict(dtrain, output_margin=True)\n",
                "ptest = bst.predict(dtest, output_margin=True)\n",
                "dtrain.set_base_margin(ptrain)\n",
                "dtest.set_base_margin(ptest)\n",
                "\n",
                "\n",
                "print('this is result of running from initial prediction')\n",
                "bst = xgb.train(param, dtrain, 1, watchlist)\n",
                "print(bst)\n",
                "for i in bst:\n",
                "    print(i)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "output_type": "error",
                    "ename": "AttributeError",
                    "evalue": "'DMatrix' object has no attribute 'ndim'",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
                        "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_448/3618774702.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mypred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mypred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;31mAttributeError\u001b[0m: 'DMatrix' object has no attribute 'ndim'"
                    ]
                }
            ],
            "source": [
                "data = np.random.rand(7, 10)\n",
                "dtest = xgb.DMatrix(data)\n",
                "print(dtest.shape)\n",
                "ypred = bst.predict(dtest)\n",
                "ypred = bst.predict(dtest, iteration_range=(0, bst.best_iteration))\n",
                "print(data)\n",
                "print(ypred)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.10 64-bit (windows store)"
        },
        "interpreter": {
            "hash": "9c14d84c7e79482e11eb99e588a0eab546d791b33bbfb86cd199096765ee886b"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}